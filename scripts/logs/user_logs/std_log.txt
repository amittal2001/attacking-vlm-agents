All args:
run_entry_2.py
/tmp/input
/mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/cap/data-capability/wd/output_53e6276e
PGDAttack
1
0
navi
evaluation_examples_windows/test_custom.json
meta-llama/Llama-3.2-11B-Vision-Instruct
pgd_attack.py
0.03
0.01
1
10
0.1
dog
905e6d9ca6ee9a10c60ee894c70083c76eb8be6a
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
oss
win32
Starting entry script...
Folder at ./:
total 40
drwxrwxrwx 6 root root 4096 Sep 11 19:39 .
drwxr-xr-x 3 root root 4096 Sep 11 19:38 ..
-rw-r--r-- 1 root root  175 Sep 11 19:39 .amlignore
drwxrwxrwx 2 root root 4096 Sep 11 19:36 azureml-logs
-rwxr-xr-x 1 root root 1378 Sep 11 19:39 compute-instance-startup.sh
drwxrwxrwx 2 root root 4096 Sep 11 19:36 logs
drwxrwxrwx 2 root root 4096 Sep 11 19:36 outputs
-rwxr-xr-x 1 root root 3657 Sep 11 19:39 run_entry.py
-rwxr-xr-x 1 root root 2723 Sep 11 19:39 run_entry_2.py
drwxrwxrwx 2 root root 4096 Sep 11 19:39 user_logs
Folder created: /mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/cap/data-capability/wd/output_53e6276e/PGDAttack
display the content of /client
total 140
drwxrwxrwx 3 root root  4096 Aug 22 09:49 data
drwxrwxrwx 6 root root  4096 Aug 15 10:53 desktop_env
drwxrwxrwx 4 root root  4096 Aug 22 09:49 evaluation_examples_windows
-rwxrwxrwx 1 root root  2890 Aug 15 10:53 human_run.py
-rwxrwxrwx 1 root root  3129 Aug 15 10:53 lib_run_single.py
drwxrwxrwx 3 root root  4096 Aug 15 10:53 mm_agents
-rwxrwxrwx 1 root root 10696 Sep 11 17:44 pgd_attack.py
-rwxrwxrwx 1 root root   832 Aug 15 10:53 requirements.txt
-rwxrwxrwx 1 root root 18111 Aug 22 09:51 run.py
-rwxrwxrwx 1 root root 23720 Sep  4 18:20 run_pgd_attack.py
-rwxrwxrwx 1 root root 23131 Sep  1 08:38 run_rand_smothing.py
-rwxrwxrwx 1 root root    30 Aug 15 10:53 settings.json
-rwxrwxrwx 1 root root  4318 Aug 15 10:53 show_frames.py
-rwxrwxrwx 1 root root  4766 Aug 15 10:53 show_result.py
-rwxrwxrwx 1 root root 10373 Aug 15 10:53 trajectory_recorder.py
wandb: No netrc file found, creating one.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: talamit2001 (talamit2001-tel-aviv-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.3
wandb: Run data is saved locally in /client/wandb/run-20250911_193929-dx9sh6op
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PGDAttack-Epsilon0.03-Alpha0.01-Steps1-TargetActiondog
wandb: â­ï¸ View project at https://wandb.ai/talamit2001-tel-aviv-university/mip-generator-attack
wandb: ðŸš€ View run at https://wandb.ai/talamit2001-tel-aviv-university/mip-generator-attack/runs/dx9sh6op
[2025-09-11 19:39:30,999] INFO Running bitsandbytes setup...
[2025-09-11 19:39:31,000] INFO Check environment...
[2025-09-11 19:39:31,000] INFO === GPU / CUDA Environment Check ===

[2025-09-11 19:39:31,000] INFO nvidia-smi output:
[2025-09-11 19:39:31,000] INFO Running: nvidia-smi
[2025-09-11 19:39:31,184] INFO Thu Sep 11 19:39:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       On  | 00000001:00:00.0 Off |                  Off |
| N/A   31C    P8               9W /  70W |      2MiB / 16384MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

[2025-09-11 19:39:31,185] INFO PyTorch CUDA info:
[2025-09-11 19:39:31,185] INFO PyTorch version: 2.4.1+cu121
[2025-09-11 19:39:31,185] INFO Compiled with CUDA: 12.1
[2025-09-11 19:39:31,558] INFO CUDA available: True
[2025-09-11 19:39:31,576] INFO CUDA device count: 1
[2025-09-11 19:39:31,580] INFO CUDA device name: Tesla T4
[2025-09-11 19:39:31,581] INFO cuDNN version: 90100
[2025-09-11 19:39:31,582] INFO Current device: 0
[2025-09-11 19:39:31,582] INFO 
>> LD_LIBRARY_PATH (for CUDA libs):
[2025-09-11 19:39:31,582] INFO Running: echo $LD_LIBRARY_PATH
[2025-09-11 19:39:31,583] INFO /usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64

[2025-09-11 19:39:31,584] INFO 
=== End of Check ===
[2025-09-11 19:39:31,584] INFO End environment check...
[2025-09-11 19:39:31,584] INFO Running bitsandbytes diagnostic...
[2025-09-11 19:39:34,838] INFO bitsandbytes stdout: ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++
CUDA specs: CUDASpecs(highest_compute_capability=(7, 5), cuda_version_string='121', cuda_version_tuple=(12, 1))
PyTorch settings found: CUDA_VERSION=121, Highest Compute Capability: (7, 5).
To manually override the PyTorch CUDA version please see: https://github.com/TimDettmers/bitsandbytes/blob/main/docs/source/nonpytorchcuda.mdx
Found duplicate CUDA runtime files (see below).

We select the PyTorch default CUDA runtime, which is 12.1,
but this might mismatch with the CUDA version that is needed for bitsandbytes.
To override this behavior set the `BNB_CUDA_VERSION=<version string, e.g. 122>` environmental variable.

For example, if you want to use the CUDA version 122,
    BNB_CUDA_VERSION=122 python ...

OR set the environmental variable in your .bashrc:
    export BNB_CUDA_VERSION=122

In the case of a manual override, make sure you set LD_LIBRARY_PATH, e.g.
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2,
* Found CUDA runtime at: /usr/local/cuda/lib64/libcudart.so.12.2.53
* Found CUDA runtime at: /usr/local/cuda/lib64/libcudart.so.12
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Checking that the library is importable and CUDA is callable...
SUCCESS!
Installation was successful!

[2025-09-11 19:39:34,838] INFO bitsandbytes stderr: The directory listed in your path is found to be non-existent: /usr/local/nvidia/lib
The directory listed in your path is found to be non-existent: /usr/local/nvidia/lib64
The directory listed in your path is found to be non-existent: /mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/.grpc/executor
The directory listed in your path is found to be non-existent: /mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/.grpc/metrics-capability
The directory listed in your path is found to be non-existent: //127.0.0.1
The directory listed in your path is found to be non-existent: 46808/MSI/auth
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms
The directory listed in your path is found to be non-existent: /mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/.grpc/snapshot-capability
The directory listed in your path is found to be non-existent: //127.0.0.1
The directory listed in your path is found to be non-existent: 46808/MSI/auth
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms","service_cert_endpoint"
The directory listed in your path is found to be non-existent: //eastus.cert.api.azureml.ms","discovery_endpoint"
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms/discovery","experiment_name"
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms","data_container_id"
The directory listed in your path is found to be non-existent: /subscriptions/85ae0eb1-7260-4b2d-9518-9ee85545143d/resourceGroups/agents/providers/Microsoft.MachineLearningServices/workspaces/agents_ml
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms/mlflow/v1.0/subscriptions/85ae0eb1-7260-4b2d-9518-9ee85545143d/resourceGroups/agents/providers/Microsoft.MachineLearningServices/workspaces/agents_ml
The directory listed in your path is found to be non-existent: "/mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/.grpc"}}
The directory listed in your path is found to be non-existent: /mnt/azureml/cr/j/35761385406b42edbcd9acb82cc16cff/.grpc/hosttools-capability
The directory listed in your path is found to be non-existent: //eastus.cert.api.azureml.ms
The directory listed in your path is found to be non-existent: /subscriptions/85ae0eb1-7260-4b2d-9518-9ee85545143d/resourceGroups/agents/providers/Microsoft.MachineLearningServices/workspaces/agents_ml/experiments/PGDAttack
The directory listed in your path is found to be non-existent: //eastus.api.azureml.ms/discovery
The directory listed in your path is found to be non-existent: 3-23-unix-/tmp/wandb-23-34-1816418222/socket

[2025-09-11 19:39:34,838] INFO End bitsandbytes diagnostic...
[2025-09-11 19:39:34,838] INFO Check environment...
[2025-09-11 19:39:34,839] INFO === GPU / CUDA Environment Check ===

[2025-09-11 19:39:34,839] INFO nvidia-smi output:
[2025-09-11 19:39:34,839] INFO Running: nvidia-smi
[2025-09-11 19:39:34,949] INFO Thu Sep 11 19:39:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       On  | 00000001:00:00.0 Off |                  Off |
| N/A   32C    P0              26W /  70W |      5MiB / 16384MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

[2025-09-11 19:39:34,949] INFO PyTorch CUDA info:
[2025-09-11 19:39:34,949] INFO PyTorch version: 2.4.1+cu121
[2025-09-11 19:39:34,949] INFO Compiled with CUDA: 12.1
[2025-09-11 19:39:34,949] INFO CUDA available: True
[2025-09-11 19:39:34,949] INFO CUDA device count: 1
[2025-09-11 19:39:34,950] INFO CUDA device name: Tesla T4
[2025-09-11 19:39:34,950] INFO cuDNN version: 90100
[2025-09-11 19:39:34,950] INFO Current device: 0
[2025-09-11 19:39:34,950] INFO 
>> LD_LIBRARY_PATH (for CUDA libs):
[2025-09-11 19:39:34,950] INFO Running: echo $LD_LIBRARY_PATH
[2025-09-11 19:39:34,952] INFO /usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64

[2025-09-11 19:39:34,952] INFO 
=== End of Check ===
[2025-09-11 19:39:34,952] INFO End environment check...
[2025-09-11 19:39:35,488] INFO Loading processor from meta-llama/Llama-3.2-11B-Vision-Instruct (local_files_only=False)
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2304.56it/s]
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3355.44it/s]
[2025-09-11 19:39:37,906] INFO Loading model meta-llama/Llama-3.2-11B-Vision-Instruct in 4-bit quantized mode on cuda dtype=torch.float16
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:  20%|â–ˆâ–ˆ        | 1/5 [09:20<37:21, 560.28s/it]Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [09:20<00:00, 112.06s/it]
[2025-09-11 19:49:05,528] INFO We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:36<02:27, 36.98s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:46<02:48, 56.01s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:11<01:24, 42.11s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [03:21<00:52, 52.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:43<00:00, 41.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:43<00:00, 44.61s/it]
[2025-09-11 19:52:49,321] INFO Use pytorch device_name: cuda:0
[2025-09-11 19:52:49,321] INFO Load pretrained SentenceTransformer: all-MiniLM-L6-v2
[2025-09-11 19:52:55,913] INFO Device: cuda
[2025-09-11 19:52:55,925] INFO Starting forward pass...
[2025-09-11 19:52:59,499] INFO Finished forward pass...
[2025-09-11 19:52:59,613] INFO Predicted text: defdef are here cat and toTheWhatThe the

 ofassistant image:

:

 The
[2025-09-11 19:53:03,742] INFO Predicted text for step 0: defdef are here cat and toTheWhatThe the

 ofassistant image:

:

 The
[2025-09-11 19:53:05,107] INFO Predicted text: defdef are here cat and toTheWhatThe the

 ofassistant image:

:

 The
wandb: uploading artifact run-dx9sh6op-model_responses; uploading artifact run-dx9sh6op-final_response_table; updating run metadata
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: loss â–
wandb: step â–â–
wandb: 
wandb: Run summary:
wandb: loss 11.58594
wandb: step 0
wandb: 
wandb: ðŸš€ View run PGDAttack-Epsilon0.03-Alpha0.01-Steps1-TargetActiondog at: https://wandb.ai/talamit2001-tel-aviv-university/mip-generator-attack/runs/dx9sh6op
wandb: â­ï¸ View project at: https://wandb.ai/talamit2001-tel-aviv-university/mip-generator-attack
wandb: Synced 5 W&B file(s), 5 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250911_193929-dx9sh6op/logs
Finished running entry script
